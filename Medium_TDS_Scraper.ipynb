{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup  \n",
    "from time import sleep\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Scraping Title, Text, and Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function to minimize clutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request1(url):\n",
    "    # MAKE HTTP POST REQUEST  \n",
    "    page = ''\n",
    "    while page == '':\n",
    "        # IN CASE OF TOO MANY REQUESTS\n",
    "        try:\n",
    "            page = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            webpage = urlopen(page)\n",
    "            if webpage.getcode() == 200:\n",
    "                break\n",
    "        except Exception as inst:\n",
    "            print(\"Connection refused by the server..\")\n",
    "            print(\"Let me sleep for 5 seconds\")\n",
    "            print(\"ZZzzzz...\")\n",
    "            sleep(30)\n",
    "            print(\"Was a nice sleep, now let me continue...\")\n",
    "            continue\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather links for articles from January 2020 to July 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n",
      "Connection refused by the server..\n",
      "Let me sleep for 5 seconds\n",
      "ZZzzzz...\n",
      "Was a nice sleep, now let me continue...\n",
      "02\n",
      "03\n",
      "04\n",
      "05\n",
      "06\n",
      "07\n",
      "11585 articles\n"
     ]
    }
   ],
   "source": [
    "# STORE HTML CODE FOR EACH PAGE\n",
    "store_pages=[] \n",
    "\n",
    "# SET TIME PARAMETERS\n",
    "months = ['01','02','03','04','05','06','07'] \n",
    "days29 = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29'] \n",
    "days30 = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30'] \n",
    "days31 = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31'] \n",
    "\n",
    "for i in months: # LOOP THROUGH MONTHS\n",
    "    \n",
    "    if i == '02': # FEBRUARY 2020 HAS 29 DAYS IN A MONTH\n",
    "        print(i)\n",
    "        for j in days29: # LOOP THROUGH DAYS\n",
    "            \n",
    "            url ='https://towardsdatascience.com/archive/2020/'+ i + '/' + j\n",
    "            page = make_request1(url)\n",
    "            \n",
    "            # PARSE PAGE\n",
    "            webpage = urlopen(page).read()\n",
    "            soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "            store_pages.append(soup) # STORE EACH HTML PARSED PAGE IN LIST \n",
    "            sleep(5)\n",
    "            \n",
    "    elif i == '04' or i == '06': # APRIL AND JUNE HAVE 30 DAYS IN A MONTH\n",
    "        print(i)\n",
    "        for j in days30: # LOOP THROUGH DAYS\n",
    "            \n",
    "            url ='https://towardsdatascience.com/archive/2020/'+ i + '/' + j\n",
    "            page = make_request1(url)\n",
    "            \n",
    "            # PARSE PAGE\n",
    "            webpage = urlopen(page).read()\n",
    "            soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "            store_pages.append(soup) # STORE EACH HTML PARSED PAGE IN LIST \n",
    "            sleep(5)\n",
    "            \n",
    "    else: # ALL OTHER MONTHS HAVE 31 DAYS \n",
    "        print(i)\n",
    "        for j in days31: # LOOP THROUGH DAYS\n",
    "            \n",
    "            url ='https://towardsdatascience.com/archive/2020/'+ i + '/' + j\n",
    "            page = make_request1(url)\n",
    "            \n",
    "            # PARSE PAGE\n",
    "            webpage = urlopen(page).read()\n",
    "            soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "            store_pages.append(soup) # STORE EACH HTML PARSED PAGE IN LIST \n",
    "            sleep(5)\n",
    "            \n",
    "# RETRIEVE LINKS\n",
    "links_tds= [] \n",
    "for i in range(0, len(store_pages)):\n",
    "    for link in store_pages[i].find_all(\"a\", {\"class\": \"button button--smaller button--chromeless u-baseColor--buttonNormal\"}):\n",
    "        links_tds.append(link.get('href'))\n",
    "print(len(links_tds),'articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull relevant data from each article (11,585 total articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article number 1\n",
      "article number 1001\n",
      "article number 2001\n",
      "article number 3001\n",
      "article number 4001\n",
      "article number 5001\n",
      "Connection refused by the server..\n",
      "Let me sleep for 30 seconds\n",
      "ZZzzzz...\n",
      "Was a nice sleep, now let me continue...\n",
      "article number 6001\n",
      "article number 7001\n",
      "article number 8001\n",
      "article number 9001\n",
      "article number 10001\n",
      "article number 11001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[/tagged/programming, /tagged/python, /tagged/...</td>\n",
       "      <td>[Python haters always say, that one of the rea...</td>\n",
       "      <td>Making Python Programs Blazingly Fast</td>\n",
       "      <td>https://towardsdatascience.com/making-python-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[/tagged/programming, /tagged/technology, /tag...</td>\n",
       "      <td>[Languages change. Languages adapt. Python 2 i...</td>\n",
       "      <td>6 New Features in Python 3.8 for Python Newbies</td>\n",
       "      <td>https://towardsdatascience.com/6-new-features-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[/tagged/python, /tagged/data-science, /tagged...</td>\n",
       "      <td>[Python is cool. Really cool. However, a lot o...</td>\n",
       "      <td>How to be fancy with Python</td>\n",
       "      <td>https://towardsdatascience.com/how-to-be-fancy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[/tagged/machine-learning, /tagged/tensorflow,...</td>\n",
       "      <td>[Convolutional neural networks (CNN) work grea...</td>\n",
       "      <td>Understanding and implementing a fully convolu...</td>\n",
       "      <td>https://towardsdatascience.com/implementing-a-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[/tagged/machine-learning, /tagged/privacy, /t...</td>\n",
       "      <td>[Data privacy has been called “the most import...</td>\n",
       "      <td>Perfectly Privacy-Preserving AI</td>\n",
       "      <td>https://towardsdatascience.com/perfectly-priva...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tags  \\\n",
       "0  [/tagged/programming, /tagged/python, /tagged/...   \n",
       "1  [/tagged/programming, /tagged/technology, /tag...   \n",
       "2  [/tagged/python, /tagged/data-science, /tagged...   \n",
       "3  [/tagged/machine-learning, /tagged/tensorflow,...   \n",
       "4  [/tagged/machine-learning, /tagged/privacy, /t...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  [Python haters always say, that one of the rea...   \n",
       "1  [Languages change. Languages adapt. Python 2 i...   \n",
       "2  [Python is cool. Really cool. However, a lot o...   \n",
       "3  [Convolutional neural networks (CNN) work grea...   \n",
       "4  [Data privacy has been called “the most import...   \n",
       "\n",
       "                                               Title  \\\n",
       "0              Making Python Programs Blazingly Fast   \n",
       "1    6 New Features in Python 3.8 for Python Newbies   \n",
       "2                        How to be fancy with Python   \n",
       "3  Understanding and implementing a fully convolu...   \n",
       "4                    Perfectly Privacy-Preserving AI   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://towardsdatascience.com/making-python-p...  \n",
       "1  https://towardsdatascience.com/6-new-features-...  \n",
       "2  https://towardsdatascience.com/how-to-be-fancy...  \n",
       "3  https://towardsdatascience.com/implementing-a-...  \n",
       "4  https://towardsdatascience.com/perfectly-priva...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STORE RESULTS\n",
    "article_info_tds=[] \n",
    "df_tds=[]\n",
    "\n",
    "for i in range(0,len(links_tds)): # LOOP THROUGH LINKS\n",
    "\n",
    "    # TO TRACK PROGRESS\n",
    "    if (i/1000) % 2 == 1 or (i/1000) % 2 == 0:\n",
    "        print('article number', i+1)\n",
    "        \n",
    "    # MAKE HTTP GET REQUEST\n",
    "    url = links_tds[i]\n",
    "    page = ''\n",
    "    while page == '':\n",
    "    # IN CASE OF TOO MANY REQUESTS\n",
    "        try:\n",
    "            page = requests.get(url)\n",
    "            if page.status_code == 200:\n",
    "                break\n",
    "        except Exception as inst:\n",
    "            print(\"Connection refused by the server..\")\n",
    "            print(\"Let me sleep for 30 seconds\")\n",
    "            print(\"ZZzzzz...\")\n",
    "            sleep(30)\n",
    "            print(\"Was a nice sleep, now let me continue...\")\n",
    "            continue\n",
    "           \n",
    "    # PARSE PAGE\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # TITLE (REMEMBER, SOME TITLES MAY NOT HAVE h1 TAG)\n",
    "    title = []\n",
    "    for my_tag in soup.find_all('h1'):\n",
    "        title.append(my_tag.text)\n",
    "    if title != []:\n",
    "        title = title[0]\n",
    "    else:\n",
    "        title = ''\n",
    "        \n",
    "#     # FIGURES\n",
    "#     figs = []\n",
    "#     for my_tag in soup.find_all('figure'):\n",
    "#         if my_tag != []:\n",
    "#             figs.append(my_tag['class'])\n",
    "#         else:\n",
    "#             figs = ''\n",
    "    \n",
    "    # TEXT\n",
    "    body = []\n",
    "    for my_tag in soup.find_all('p'):\n",
    "        body.append(my_tag.text)\n",
    "    \n",
    "    # TAGS\n",
    "    tags = []\n",
    "    for my_tag in soup.find_all('a', href=True):\n",
    "        if my_tag['href'].startswith('/tagged'): # ALL TAGS HAVE '/tagged' PREFIX\n",
    "            tags.append(my_tag['href'])\n",
    "    \n",
    "    # LINK\n",
    "    link = url\n",
    "     \n",
    "    # ACCUMULATE DATA INTO A DICTIONARY \n",
    "    article_info_tds = {'Title':title,'Text':body,'Tags':tags,'Url':link} \n",
    "\n",
    "    # STORE RESULTS IN LIST\n",
    "    df_tds.append(article_info_tds)\n",
    "    sleep(5)\n",
    "\n",
    "# CONVERT LIST TO DATAFRAME\n",
    "df1 = pd.DataFrame(df_tds) \n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Scraping Subtitles, Upvotes, Date, Name, Read Time, and Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions for second scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request2(url):\n",
    "# MAKE REQUEST  \n",
    "    driver = ''\n",
    "    while driver == '':\n",
    "        try:\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(url)\n",
    "            res = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "            driver.quit()\n",
    "            break\n",
    "        except:\n",
    "            sleep(30)\n",
    "            continue\n",
    "    return res\n",
    "\n",
    "def find_post_cards(soup):\n",
    "    # PULLS EACH CARD FROM THE FEED. EACH CARD IS A STORY OR COMMENT\n",
    "    cards = soup.find_all(\"div\", class_=\"streamItem streamItem--postPreview js-streamItem\")\n",
    "    return cards\n",
    "\n",
    "def get_subtitles_from_cards(cards):\n",
    "    # PULLS TITLE DATA FROM EACH CARD IN CARDS, RETURNS A LIST OF TITLES\n",
    "    def subtitle_cleaner(subtitle):\n",
    "        # REMOVE MEDIUMS ENCODING SYMBOLS AND EMOJIS FROM TITLES\n",
    "        subtitle = subtitle.replace(\"\\xa0\",\" \")\n",
    "        subtitle = subtitle.replace(\"\\u200a\",\"\")\n",
    "        subtitle = subtitle.replace(\"\\ufe0f\",\"\")\n",
    "        subtitle = re.sub(r'[^\\x00-\\x7F]+','', subtitle)\n",
    "        return subtitle\n",
    "\n",
    "    subtitles=[]\n",
    "    for card in cards:\n",
    "        # SEARCH FOR TITLE THERE ARE 3 DIFF CLASSES\n",
    "        variant1 = card.find(\"h4\", class_=\"graf graf--h4 graf-after--h3 graf--subtitle\")\n",
    "        variant2 = card.find(\"h4\", class_=\"graf graf--h4 graf-after--h3 graf--trailing graf--subtitle\")\n",
    "        variant3 = card.find(\"strong\", class_=\"markup--strong markup--p-strong\")\n",
    "        variant4 = card.find(\"h4\", class_=\"graf graf--p graf-after--h3 graf--trailing\")\n",
    "        variant5= card.find(\"p\", class_=\"graf graf--p graf-after--h3 graf--trailing\")\n",
    "        variant6= card.find(\"blockquote\", class_=\"graf graf--pullquote graf-after--figure graf--trailing\")\n",
    "        variant7 = card.find(\"p\", class_=\"graf graf--p graf-after--figure\")\n",
    "        variant8 = card.find(\"blockquote\", class_=\"graf graf--blockquote graf-after--h3 graf--trailing\")\n",
    "        variant9 = card.find(\"p\", class_=\"graf graf--p graf-after--figure graf--trailing\")\n",
    "        variant10 = card.find(\"em\", class_=\"markup--em markup--p-em\")\n",
    "        variant11=card.find(\"p\", class_=\"graf graf--p graf-after--p graf--trailing\")\n",
    "        # EACH CARD MUST HAVE ONE OF THE TITLE CLASSES FIND IT AND CUT OUT MEDIUM'S\n",
    "        # STYLING CODES\n",
    "        variants = [variant1, variant2, variant3, variant4, variant5, variant6, variant7, variant8, variant9, variant10, variant11]\n",
    "        saved = False\n",
    "        for variant in variants:\n",
    "            if ((variant is not None) and (not saved)):\n",
    "                subtitle = variant.text\n",
    "                subtitle = subtitle_cleaner(subtitle)\n",
    "                subtitles.append(subtitle)\n",
    "                saved = True\n",
    "        if not saved:\n",
    "            subtitles.append(\"NaN\")\n",
    "    return subtitles \n",
    "\n",
    "def get_urls_from_cards(cards):\n",
    "    # GETS ARTICLE URLS FROM ALL CARDS\n",
    "    urls = []\n",
    "    for card in cards:\n",
    "        url = card.find(\"a\", class_=\"\")\n",
    "        if url is not None:\n",
    "            urls.append(url['href'])\n",
    "        else:\n",
    "            raise Exception(\"couldnt find a url\")\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather subtitle, upvotes, date, name, read time, and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n",
      "02\n",
      "03\n",
      "04\n",
      "05\n",
      "06\n",
      "07\n"
     ]
    }
   ],
   "source": [
    "# STORE RESULTS\n",
    "title=[]\n",
    "subtitle=[]\n",
    "link=[]\n",
    "upvotes=[]\n",
    "date=[]\n",
    "name=[]\n",
    "read=[]\n",
    "comments=[]\n",
    "\n",
    "for i in months: # LOOP THROUGH MONTHS\n",
    "    \n",
    "    if i == '02': # FEBRUARY 2020 HAS 29 DAYS IN A MONTH\n",
    "        print(i)\n",
    "        for j in days29: # LOOP THROUGH DAYS\n",
    "\n",
    "            url = 'https://towardsdatascience.com/archive/2020/'+ i + '/' + j\n",
    "            res = make_request2(url)\n",
    "                \n",
    "            # PARSE PAGE\n",
    "            soup = BeautifulSoup(res, 'lxml')\n",
    "            cards = find_post_cards(soup)\n",
    "            \n",
    "            # SUBTITLE\n",
    "            subtitle.append(get_subtitles_from_cards(cards))\n",
    "            \n",
    "            # URL\n",
    "            link.append(get_urls_from_cards(cards))\n",
    "                \n",
    "            # UPVOTES\n",
    "            for my_tag in soup.find_all('span',{'class':'u-relative u-background js-actionMultirecommendCount u-marginLeft5'}):\n",
    "                upvotes.append(my_tag.text)\n",
    "            \n",
    "            # DATE\n",
    "            for my_tag in soup.findAll('time'):\n",
    "                if my_tag.has_attr('datetime'):\n",
    "                    date.append(my_tag['datetime'])\n",
    "                else:\n",
    "                    date.append(my_tag.text)\n",
    "                    \n",
    "            # NAME\n",
    "            for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken\"):\n",
    "                name.append(my_tag.text)\n",
    "             \n",
    "            # READ\n",
    "            for my_tag in soup.find_all(class_=\"readingTime\"):\n",
    "                read.append(my_tag.get('title'))\n",
    "\n",
    "            # COMMENTS\n",
    "            for my_tag in soup.find_all('div',{'class':'buttonSet u-floatRight'}):\n",
    "                comments.append(my_tag.text)\n",
    "                \n",
    "            sleep(10)\n",
    "\n",
    "    elif i == '04' or i == '06': # APRIL AND JUNE HAVE 30 DAYS IN A MONTH\n",
    "        print(i)\n",
    "        for j in days30: # LOOP THROUGH DAYS\n",
    "\n",
    "            url = 'https://towardsdatascience.com/archive/2020/'+ i + '/' + j\n",
    "            res = make_request2(url)\n",
    "                    \n",
    "            # PARSE PAGE\n",
    "            soup = BeautifulSoup(res, 'lxml')\n",
    "            cards = find_post_cards(soup)\n",
    "\n",
    "            # SUBTITLE\n",
    "            subtitle.append(get_subtitles_from_cards(cards))          \n",
    "\n",
    "            # URL\n",
    "            link.append(get_urls_from_cards(cards))\n",
    "            \n",
    "            # UPVOTES\n",
    "            for my_tag in soup.find_all('span',{'class':'u-relative u-background js-actionMultirecommendCount u-marginLeft5'}):\n",
    "                upvotes.append(my_tag.text)\n",
    "    \n",
    "            # DATE\n",
    "            for my_tag in soup.findAll('time'):\n",
    "                if my_tag.has_attr('datetime'):\n",
    "                    date.append(my_tag['datetime'])\n",
    "                else:\n",
    "                    date.append(my_tag.text)\n",
    "            \n",
    "            # NAME\n",
    "            for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken\"):\n",
    "                name.append(my_tag.text)\n",
    "                \n",
    "            # READ              \n",
    "            for my_tag in soup.find_all(class_=\"readingTime\"):\n",
    "                read.append(my_tag.get('title'))\n",
    " \n",
    "            # COMMENTS\n",
    "            for my_tag in soup.find_all('div',{'class':'buttonSet u-floatRight'}):\n",
    "                comments.append(my_tag.text)\n",
    "                \n",
    "            sleep(10)\n",
    "        \n",
    "    else: # ALL OTHER MONTHS HAVE 31 DAYS \n",
    "        print(i)\n",
    "        for j in days31: # LOOP THROUGH DAYS\n",
    "\n",
    "            url = 'https://towardsdatascience.com/archive/2020/'+ i + '/' + j\n",
    "            res = make_request2(url)\n",
    "                    \n",
    "            # PARSE PAGE\n",
    "            soup = BeautifulSoup(res, 'lxml')\n",
    "            cards = find_post_cards(soup)\n",
    "            \n",
    "            # SUBTITLE\n",
    "            subtitle.append(get_subtitles_from_cards(cards))\n",
    "            \n",
    "            # URL\n",
    "            link.append(get_urls_from_cards(cards))\n",
    "                    \n",
    "            # UPVOTES\n",
    "            for my_tag in soup.find_all('span',{'class':'u-relative u-background js-actionMultirecommendCount u-marginLeft5'}):\n",
    "                upvotes.append(my_tag.text)\n",
    "    \n",
    "            # DATE\n",
    "            for my_tag in soup.findAll('time'):\n",
    "                if my_tag.has_attr('datetime'):\n",
    "                    date.append(my_tag['datetime'])\n",
    "                else:\n",
    "                    date.append(my_tag.text)\n",
    "            \n",
    "            # NAME\n",
    "            for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken\"):\n",
    "                name.append(my_tag.text)\n",
    "\n",
    "            # READ\n",
    "            for my_tag in soup.find_all(class_=\"readingTime\"):\n",
    "                read.append(my_tag.get('title'))\n",
    "                \n",
    "            # COMMENTS\n",
    "            for my_tag in soup.find_all('div',{'class':'buttonSet u-floatRight'}):\n",
    "                comments.append(my_tag.text)\n",
    "                \n",
    "            sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten title, subtitle, and url lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_subtitle = [y for x in subtitle for y in x]\n",
    "flattened_link = [y for x in link for y in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Url</th>\n",
       "      <th>Name</th>\n",
       "      <th>Upvotes</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Read</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lets look at the performance of our Python pro...</td>\n",
       "      <td>https://towardsdatascience.com/making-python-p...</td>\n",
       "      <td>Martin Heinz</td>\n",
       "      <td>3.3K</td>\n",
       "      <td>2020-01-01T20:15:03.352Z</td>\n",
       "      <td>4 responses</td>\n",
       "      <td>5 min read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prepare yourself as</td>\n",
       "      <td>https://towardsdatascience.com/6-new-features-...</td>\n",
       "      <td>Eden Au</td>\n",
       "      <td>1.7K</td>\n",
       "      <td>2020-01-01T23:27:59.289Z</td>\n",
       "      <td>4 responses</td>\n",
       "      <td>4 min read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python tricks that will make your life easier</td>\n",
       "      <td>https://towardsdatascience.com/how-to-be-fancy...</td>\n",
       "      <td>Dipam Vasani</td>\n",
       "      <td>1.7K</td>\n",
       "      <td>2020-01-01T14:26:52.211Z</td>\n",
       "      <td>12 responses</td>\n",
       "      <td>5 min read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A tutorial on building, training and</td>\n",
       "      <td>https://towardsdatascience.com/implementing-a-...</td>\n",
       "      <td>Himanshu Rawlani</td>\n",
       "      <td>271</td>\n",
       "      <td>2020-01-01T16:01:59.680Z</td>\n",
       "      <td>1 response</td>\n",
       "      <td>10 min read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is it and how do we achieve it?</td>\n",
       "      <td>https://towardsdatascience.com/perfectly-priva...</td>\n",
       "      <td>Patricia Thaine</td>\n",
       "      <td>206</td>\n",
       "      <td>2020-01-01T23:46:51.168Z</td>\n",
       "      <td>1 response</td>\n",
       "      <td>10 min read</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Subtitle  \\\n",
       "0  Lets look at the performance of our Python pro...   \n",
       "1                                Prepare yourself as   \n",
       "2      Python tricks that will make your life easier   \n",
       "3               A tutorial on building, training and   \n",
       "4               What is it and how do we achieve it?   \n",
       "\n",
       "                                                 Url              Name  \\\n",
       "0  https://towardsdatascience.com/making-python-p...      Martin Heinz   \n",
       "1  https://towardsdatascience.com/6-new-features-...           Eden Au   \n",
       "2  https://towardsdatascience.com/how-to-be-fancy...      Dipam Vasani   \n",
       "3  https://towardsdatascience.com/implementing-a-...  Himanshu Rawlani   \n",
       "4  https://towardsdatascience.com/perfectly-priva...   Patricia Thaine   \n",
       "\n",
       "  Upvotes                      Date      Comments         Read  \n",
       "0    3.3K  2020-01-01T20:15:03.352Z   4 responses   5 min read  \n",
       "1    1.7K  2020-01-01T23:27:59.289Z   4 responses   4 min read  \n",
       "2    1.7K  2020-01-01T14:26:52.211Z  12 responses   5 min read  \n",
       "3     271  2020-01-01T16:01:59.680Z    1 response  10 min read  \n",
       "4     206  2020-01-01T23:46:51.168Z    1 response  10 min read  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'Subtitle':flattened_subtitle,'Url':flattened_link,'Name':name,'Upvotes':upvotes,'Date':date,'Comments':comments, \"Read\": read})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Combine Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>Url</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Name</th>\n",
       "      <th>Upvotes</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Read</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[/tagged/programming, /tagged/python, /tagged/...</td>\n",
       "      <td>[Python haters always say, that one of the rea...</td>\n",
       "      <td>Making Python Programs Blazingly Fast</td>\n",
       "      <td>https://towardsdatascience.com/making-python-p...</td>\n",
       "      <td>Lets look at the performance of our Python pro...</td>\n",
       "      <td>Martin Heinz</td>\n",
       "      <td>3.3K</td>\n",
       "      <td>2020-01-01T20:15:03.352Z</td>\n",
       "      <td>4 responses</td>\n",
       "      <td>5 min read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[/tagged/programming, /tagged/technology, /tag...</td>\n",
       "      <td>[Languages change. Languages adapt. Python 2 i...</td>\n",
       "      <td>6 New Features in Python 3.8 for Python Newbies</td>\n",
       "      <td>https://towardsdatascience.com/6-new-features-...</td>\n",
       "      <td>Prepare yourself as</td>\n",
       "      <td>Eden Au</td>\n",
       "      <td>1.7K</td>\n",
       "      <td>2020-01-01T23:27:59.289Z</td>\n",
       "      <td>4 responses</td>\n",
       "      <td>4 min read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[/tagged/python, /tagged/data-science, /tagged...</td>\n",
       "      <td>[Python is cool. Really cool. However, a lot o...</td>\n",
       "      <td>How to be fancy with Python</td>\n",
       "      <td>https://towardsdatascience.com/how-to-be-fancy...</td>\n",
       "      <td>Python tricks that will make your life easier</td>\n",
       "      <td>Dipam Vasani</td>\n",
       "      <td>1.7K</td>\n",
       "      <td>2020-01-01T14:26:52.211Z</td>\n",
       "      <td>12 responses</td>\n",
       "      <td>5 min read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[/tagged/machine-learning, /tagged/tensorflow,...</td>\n",
       "      <td>[Convolutional neural networks (CNN) work grea...</td>\n",
       "      <td>Understanding and implementing a fully convolu...</td>\n",
       "      <td>https://towardsdatascience.com/implementing-a-...</td>\n",
       "      <td>A tutorial on building, training and</td>\n",
       "      <td>Himanshu Rawlani</td>\n",
       "      <td>271</td>\n",
       "      <td>2020-01-01T16:01:59.680Z</td>\n",
       "      <td>1 response</td>\n",
       "      <td>10 min read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[/tagged/machine-learning, /tagged/privacy, /t...</td>\n",
       "      <td>[Data privacy has been called “the most import...</td>\n",
       "      <td>Perfectly Privacy-Preserving AI</td>\n",
       "      <td>https://towardsdatascience.com/perfectly-priva...</td>\n",
       "      <td>What is it and how do we achieve it?</td>\n",
       "      <td>Patricia Thaine</td>\n",
       "      <td>206</td>\n",
       "      <td>2020-01-01T23:46:51.168Z</td>\n",
       "      <td>1 response</td>\n",
       "      <td>10 min read</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tags  \\\n",
       "0  [/tagged/programming, /tagged/python, /tagged/...   \n",
       "1  [/tagged/programming, /tagged/technology, /tag...   \n",
       "2  [/tagged/python, /tagged/data-science, /tagged...   \n",
       "3  [/tagged/machine-learning, /tagged/tensorflow,...   \n",
       "4  [/tagged/machine-learning, /tagged/privacy, /t...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  [Python haters always say, that one of the rea...   \n",
       "1  [Languages change. Languages adapt. Python 2 i...   \n",
       "2  [Python is cool. Really cool. However, a lot o...   \n",
       "3  [Convolutional neural networks (CNN) work grea...   \n",
       "4  [Data privacy has been called “the most import...   \n",
       "\n",
       "                                               Title  \\\n",
       "0              Making Python Programs Blazingly Fast   \n",
       "1    6 New Features in Python 3.8 for Python Newbies   \n",
       "2                        How to be fancy with Python   \n",
       "3  Understanding and implementing a fully convolu...   \n",
       "4                    Perfectly Privacy-Preserving AI   \n",
       "\n",
       "                                                 Url  \\\n",
       "0  https://towardsdatascience.com/making-python-p...   \n",
       "1  https://towardsdatascience.com/6-new-features-...   \n",
       "2  https://towardsdatascience.com/how-to-be-fancy...   \n",
       "3  https://towardsdatascience.com/implementing-a-...   \n",
       "4  https://towardsdatascience.com/perfectly-priva...   \n",
       "\n",
       "                                            Subtitle              Name  \\\n",
       "0  Lets look at the performance of our Python pro...      Martin Heinz   \n",
       "1                                Prepare yourself as           Eden Au   \n",
       "2      Python tricks that will make your life easier      Dipam Vasani   \n",
       "3               A tutorial on building, training and  Himanshu Rawlani   \n",
       "4               What is it and how do we achieve it?   Patricia Thaine   \n",
       "\n",
       "  Upvotes                      Date      Comments         Read  \n",
       "0    3.3K  2020-01-01T20:15:03.352Z   4 responses   5 min read  \n",
       "1    1.7K  2020-01-01T23:27:59.289Z   4 responses   4 min read  \n",
       "2    1.7K  2020-01-01T14:26:52.211Z  12 responses   5 min read  \n",
       "3     271  2020-01-01T16:01:59.680Z    1 response  10 min read  \n",
       "4     206  2020-01-01T23:46:51.168Z    1 response  10 min read  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First remove '-----' from URL to make URLs same across both dataframes\n",
    "df1['Url'] = df1['Url'].str.split('-----').str[0]\n",
    "df2['Url'] = df2['Url'].str.split('-----').str[0]\n",
    "\n",
    "# Merge\n",
    "df_m = pd.merge(left=df1, right=df2, left_on='Url', right_on='Url')\n",
    "df_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/halabanz/Desktop/tds_analysis/df.csv\"\n",
    "df_m.to_csv(path, index = None, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
